{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AsUVw2WIDwtp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93d4ade9-9c37-4358-a379-af30a9934fc6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./mnist_data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 185245287.05it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./mnist_data/MNIST/raw/train-images-idx3-ubyte.gz to ./mnist_data/MNIST/raw\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./mnist_data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 38726244.83it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./mnist_data/MNIST/raw/train-labels-idx1-ubyte.gz to ./mnist_data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./mnist_data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 107271352.96it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./mnist_data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./mnist_data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./mnist_data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 4542/4542 [00:00<00:00, 19360293.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./mnist_data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./mnist_data/MNIST/raw\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.autograd import Variable\n",
        "from torchvision.utils import save_image\n",
        "from google.colab import files\n",
        "\n",
        "batch_size = 100\n",
        "train_dataset = datasets.MNIST(root='./mnist_data/', train=True, transform=transforms.ToTensor(), download=True)\n",
        "test_dataset = datasets.MNIST(root='./mnist_data/', train=False, transform=transforms.ToTensor(), download=False)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class VAE(nn.Module):\n",
        "    def __init__(self, x, h1, h2, z):\n",
        "        super(VAE, self).__init__()\n",
        "\n",
        "        # encoder layers\n",
        "        self.e1 = nn.Linear(x, h1)\n",
        "        self.e2 = nn.Linear(h1, h2)\n",
        "        self.e31 = nn.Linear(h2, z)\n",
        "        self.e32 = nn.Linear(h2, z)\n",
        "\n",
        "        # decoder layers\n",
        "        self.d1 = nn.Linear(z, h2)\n",
        "        self.d2 = nn.Linear(h2, h1)\n",
        "        self.d3 = nn.Linear(h1, x)\n",
        "\n",
        "    # compiling encoder\n",
        "    def encoder(self, _x):\n",
        "        _h = F.relu(self.e1(_x))\n",
        "        _h = F.relu(self.e2(_h))\n",
        "        return self.e31(_h), self.e32(_h)\n",
        "\n",
        "    # return z sample\n",
        "    def sampling(self, mu, log_var):\n",
        "        std = torch.exp(0.5*log_var)\n",
        "        eps = torch.randn_like(std)\n",
        "        return eps.mul(std).add_(mu)\n",
        "\n",
        "    # compiling decoder\n",
        "    def decoder(self, _z):\n",
        "        _h = F.relu(self.d1(_z))\n",
        "        _h = F.relu(self.d2(_h))\n",
        "        return F.sigmoid(self.d3(_h))\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, log_var = self.encoder(x.view(-1, 784))\n",
        "        _z = self.sampling(mu, log_var)\n",
        "        return self.decoder(_z), mu, log_var\n",
        "\n",
        "# build model\n",
        "vae = VAE(784, 1024, 512, 2)\n",
        "if torch.cuda.is_available():\n",
        "    vae.cuda()"
      ],
      "metadata": {
        "id": "Q3JNFPPvD6as"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.Adam(vae.parameters())\n",
        "def loss_function(recon_x, x, mu, log_var):\n",
        "    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 784), reduction='sum')\n",
        "    KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
        "    return BCE + KLD"
      ],
      "metadata": {
        "id": "vSdU5jbYD_Js"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(epoch):\n",
        "    vae.train()\n",
        "    train_loss = 0\n",
        "    for data, _ in train_loader:\n",
        "\n",
        "        data = data.cuda()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        recon_batch, mu, log_var = vae(data)\n",
        "        loss = loss_function(recon_batch, data, mu, log_var)\n",
        "\n",
        "        loss.backward()\n",
        "        train_loss += loss.item()\n",
        "        optimizer.step()\n",
        "\n",
        "    print('====> Epoch: {} Average loss: {:.4f}'.format(epoch, train_loss / len(train_loader.dataset)))"
      ],
      "metadata": {
        "id": "yexUxkywEDSL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test():\n",
        "    vae.eval()\n",
        "    test_loss= 0\n",
        "    with torch.no_grad():\n",
        "        for data, _ in test_loader:\n",
        "            data = data.cuda()\n",
        "            recon, mu, log_var = vae(data)\n",
        "\n",
        "            test_loss += loss_function(recon, data, mu, log_var).item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print('====> Test set loss: {:.4f}'.format(test_loss))"
      ],
      "metadata": {
        "id": "nUKdUZmHEHf7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(1, 101):\n",
        "    train(epoch)\n",
        "    test()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mHo-c1e3ELV6",
        "outputId": "4011ebb6-0ba7-4c60-ca0a-372a6eabf443"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "====> Epoch: 1 Average loss: 172.1103\n",
            "====> Test set loss: 156.8260\n",
            "====> Epoch: 2 Average loss: 154.2797\n",
            "====> Test set loss: 152.1606\n",
            "====> Epoch: 3 Average loss: 150.5117\n",
            "====> Test set loss: 149.9285\n",
            "====> Epoch: 4 Average loss: 148.2230\n",
            "====> Test set loss: 148.3414\n",
            "====> Epoch: 5 Average loss: 146.5997\n",
            "====> Test set loss: 147.1026\n",
            "====> Epoch: 6 Average loss: 145.3655\n",
            "====> Test set loss: 145.5344\n",
            "====> Epoch: 7 Average loss: 144.2328\n",
            "====> Test set loss: 144.9117\n",
            "====> Epoch: 8 Average loss: 143.6316\n",
            "====> Test set loss: 144.4136\n",
            "====> Epoch: 9 Average loss: 142.7266\n",
            "====> Test set loss: 143.2856\n",
            "====> Epoch: 10 Average loss: 142.0439\n",
            "====> Test set loss: 142.4132\n",
            "====> Epoch: 11 Average loss: 141.7561\n",
            "====> Test set loss: 142.7780\n",
            "====> Epoch: 12 Average loss: 141.3691\n",
            "====> Test set loss: 142.2707\n",
            "====> Epoch: 13 Average loss: 140.8531\n",
            "====> Test set loss: 141.4252\n",
            "====> Epoch: 14 Average loss: 140.3999\n",
            "====> Test set loss: 141.0605\n",
            "====> Epoch: 15 Average loss: 140.1481\n",
            "====> Test set loss: 141.0777\n",
            "====> Epoch: 16 Average loss: 139.5433\n",
            "====> Test set loss: 140.5138\n",
            "====> Epoch: 17 Average loss: 139.2425\n",
            "====> Test set loss: 140.2864\n",
            "====> Epoch: 18 Average loss: 138.9793\n",
            "====> Test set loss: 139.9613\n",
            "====> Epoch: 19 Average loss: 138.6511\n",
            "====> Test set loss: 139.8648\n",
            "====> Epoch: 20 Average loss: 138.2975\n",
            "====> Test set loss: 139.9013\n",
            "====> Epoch: 21 Average loss: 138.3247\n",
            "====> Test set loss: 139.0892\n",
            "====> Epoch: 22 Average loss: 138.3367\n",
            "====> Test set loss: 138.5537\n",
            "====> Epoch: 23 Average loss: 137.7773\n",
            "====> Test set loss: 139.3328\n",
            "====> Epoch: 24 Average loss: 137.5998\n",
            "====> Test set loss: 139.5348\n",
            "====> Epoch: 25 Average loss: 137.9000\n",
            "====> Test set loss: 139.3792\n",
            "====> Epoch: 26 Average loss: 137.5659\n",
            "====> Test set loss: 139.3826\n",
            "====> Epoch: 27 Average loss: 137.1520\n",
            "====> Test set loss: 138.8225\n",
            "====> Epoch: 28 Average loss: 136.8884\n",
            "====> Test set loss: 139.6209\n",
            "====> Epoch: 29 Average loss: 137.0815\n",
            "====> Test set loss: 138.4456\n",
            "====> Epoch: 30 Average loss: 136.9917\n",
            "====> Test set loss: 138.3112\n",
            "====> Epoch: 31 Average loss: 136.8743\n",
            "====> Test set loss: 138.1756\n",
            "====> Epoch: 32 Average loss: 136.4973\n",
            "====> Test set loss: 138.2985\n",
            "====> Epoch: 33 Average loss: 136.2572\n",
            "====> Test set loss: 138.2591\n",
            "====> Epoch: 34 Average loss: 136.3226\n",
            "====> Test set loss: 138.1473\n",
            "====> Epoch: 35 Average loss: 136.0833\n",
            "====> Test set loss: 138.2152\n",
            "====> Epoch: 36 Average loss: 136.0135\n",
            "====> Test set loss: 138.3586\n",
            "====> Epoch: 37 Average loss: 136.1119\n",
            "====> Test set loss: 138.1704\n",
            "====> Epoch: 38 Average loss: 135.7120\n",
            "====> Test set loss: 137.8664\n",
            "====> Epoch: 39 Average loss: 135.7854\n",
            "====> Test set loss: 138.1590\n",
            "====> Epoch: 40 Average loss: 135.5756\n",
            "====> Test set loss: 138.0436\n",
            "====> Epoch: 41 Average loss: 135.7150\n",
            "====> Test set loss: 137.9080\n",
            "====> Epoch: 42 Average loss: 135.6292\n",
            "====> Test set loss: 138.2608\n",
            "====> Epoch: 43 Average loss: 135.3523\n",
            "====> Test set loss: 137.9287\n",
            "====> Epoch: 44 Average loss: 135.5466\n",
            "====> Test set loss: 138.3524\n",
            "====> Epoch: 45 Average loss: 135.0907\n",
            "====> Test set loss: 137.6671\n",
            "====> Epoch: 46 Average loss: 135.3565\n",
            "====> Test set loss: 137.6070\n",
            "====> Epoch: 47 Average loss: 134.9326\n",
            "====> Test set loss: 137.1420\n",
            "====> Epoch: 48 Average loss: 134.8507\n",
            "====> Test set loss: 137.6302\n",
            "====> Epoch: 49 Average loss: 134.7067\n",
            "====> Test set loss: 138.3703\n",
            "====> Epoch: 50 Average loss: 134.6208\n",
            "====> Test set loss: 137.3813\n",
            "====> Epoch: 51 Average loss: 134.5335\n",
            "====> Test set loss: 137.3084\n",
            "====> Epoch: 52 Average loss: 134.5066\n",
            "====> Test set loss: 137.5222\n",
            "====> Epoch: 53 Average loss: 134.3116\n",
            "====> Test set loss: 137.0904\n",
            "====> Epoch: 54 Average loss: 134.3517\n",
            "====> Test set loss: 138.2926\n",
            "====> Epoch: 55 Average loss: 134.7186\n",
            "====> Test set loss: 137.4346\n",
            "====> Epoch: 56 Average loss: 134.4059\n",
            "====> Test set loss: 136.9106\n",
            "====> Epoch: 57 Average loss: 134.2235\n",
            "====> Test set loss: 137.6954\n",
            "====> Epoch: 58 Average loss: 134.1156\n",
            "====> Test set loss: 136.9505\n",
            "====> Epoch: 59 Average loss: 134.0669\n",
            "====> Test set loss: 137.5680\n",
            "====> Epoch: 60 Average loss: 133.8983\n",
            "====> Test set loss: 136.9540\n",
            "====> Epoch: 61 Average loss: 134.2885\n",
            "====> Test set loss: 137.3388\n",
            "====> Epoch: 62 Average loss: 134.0566\n",
            "====> Test set loss: 137.2931\n",
            "====> Epoch: 63 Average loss: 133.9074\n",
            "====> Test set loss: 136.7417\n",
            "====> Epoch: 64 Average loss: 133.7461\n",
            "====> Test set loss: 137.3527\n",
            "====> Epoch: 65 Average loss: 133.5852\n",
            "====> Test set loss: 136.9700\n",
            "====> Epoch: 66 Average loss: 133.5754\n",
            "====> Test set loss: 136.8455\n",
            "====> Epoch: 67 Average loss: 133.7042\n",
            "====> Test set loss: 136.5063\n",
            "====> Epoch: 68 Average loss: 133.3599\n",
            "====> Test set loss: 136.9466\n",
            "====> Epoch: 69 Average loss: 133.5205\n",
            "====> Test set loss: 136.7085\n",
            "====> Epoch: 70 Average loss: 133.3783\n",
            "====> Test set loss: 136.5828\n",
            "====> Epoch: 71 Average loss: 133.1625\n",
            "====> Test set loss: 137.2265\n",
            "====> Epoch: 72 Average loss: 133.3359\n",
            "====> Test set loss: 136.5539\n",
            "====> Epoch: 73 Average loss: 133.4579\n",
            "====> Test set loss: 136.9899\n",
            "====> Epoch: 74 Average loss: 133.2432\n",
            "====> Test set loss: 136.6281\n",
            "====> Epoch: 75 Average loss: 132.9137\n",
            "====> Test set loss: 136.3893\n",
            "====> Epoch: 76 Average loss: 133.1335\n",
            "====> Test set loss: 136.8087\n",
            "====> Epoch: 77 Average loss: 132.9988\n",
            "====> Test set loss: 136.4802\n",
            "====> Epoch: 78 Average loss: 132.9610\n",
            "====> Test set loss: 137.1055\n",
            "====> Epoch: 79 Average loss: 132.8130\n",
            "====> Test set loss: 136.8313\n",
            "====> Epoch: 80 Average loss: 132.9247\n",
            "====> Test set loss: 136.6290\n",
            "====> Epoch: 81 Average loss: 133.1030\n",
            "====> Test set loss: 137.3179\n",
            "====> Epoch: 82 Average loss: 133.2361\n",
            "====> Test set loss: 136.9959\n",
            "====> Epoch: 83 Average loss: 132.9932\n",
            "====> Test set loss: 136.6418\n",
            "====> Epoch: 84 Average loss: 133.1805\n",
            "====> Test set loss: 136.4623\n",
            "====> Epoch: 85 Average loss: 132.7199\n",
            "====> Test set loss: 136.4145\n",
            "====> Epoch: 86 Average loss: 132.7797\n",
            "====> Test set loss: 136.7259\n",
            "====> Epoch: 87 Average loss: 132.8058\n",
            "====> Test set loss: 136.7620\n",
            "====> Epoch: 88 Average loss: 132.8169\n",
            "====> Test set loss: 136.5454\n",
            "====> Epoch: 89 Average loss: 132.3879\n",
            "====> Test set loss: 136.7841\n",
            "====> Epoch: 90 Average loss: 132.7793\n",
            "====> Test set loss: 136.6508\n",
            "====> Epoch: 91 Average loss: 132.5264\n",
            "====> Test set loss: 135.9506\n",
            "====> Epoch: 92 Average loss: 132.1424\n",
            "====> Test set loss: 136.6350\n",
            "====> Epoch: 93 Average loss: 132.4370\n",
            "====> Test set loss: 136.6005\n",
            "====> Epoch: 94 Average loss: 132.2604\n",
            "====> Test set loss: 136.4560\n",
            "====> Epoch: 95 Average loss: 132.4330\n",
            "====> Test set loss: 137.0269\n",
            "====> Epoch: 96 Average loss: 132.7071\n",
            "====> Test set loss: 136.9433\n",
            "====> Epoch: 97 Average loss: 132.1866\n",
            "====> Test set loss: 136.5958\n",
            "====> Epoch: 98 Average loss: 132.4374\n",
            "====> Test set loss: 136.5285\n",
            "====> Epoch: 99 Average loss: 132.3941\n",
            "====> Test set loss: 136.6901\n",
            "====> Epoch: 100 Average loss: 132.5658\n",
            "====> Test set loss: 136.6811\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    z = torch.randn(64, 2).cuda()\n",
        "    sample = vae.decoder(z).cuda()\n",
        "\n",
        "    save_image(sample.view(64, 1, 28, 28), 'sample_' + '.png')\n",
        "    files.download(\"sample_.png\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "mGrrei1VMA8e",
        "outputId": "bb709ab0-0f8b-4dc6-8d0d-ade7a04b2a2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_f0b4a5fb-3717-48fc-a78a-9fb25447a0dc\", \"sample_.png\", 43859)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}